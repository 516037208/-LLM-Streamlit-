# Streamlit 智能聊天助手应用说明书

## 1. 项目概述

Streamlit 智能聊天助手是一款基于本地大型语言模型（LLM）的交互式聊天应用，利用 Ollama 提供的本地 LLM 服务，为用户提供智能对话体验。

### 1.1 核心价值

- **隐私保护**：完全本地运行，数据不离开本地环境
- **智能交互**：支持模型思考过程显示，增加对话透明度
- **灵活配置**：支持多种模型选择，适应不同硬件条件
- **用户友好**：直观的界面设计，易于操作

### 1.2 技术架构

| 层级 | 组件 | 功能 |
|------|------|------|
| 前端 | Streamlit | 提供用户界面，处理用户输入输出 |
| 后端 | Ollama | 本地 LLM 服务，执行模型推理 |
| 模型 | DeepSeek R1 | 提供智能对话能力 |
| 存储 | 本地文件 | 存储聊天记录和配置 |

## 2. 功能特点

### 2.1 主要功能

| 功能 | 描述 | 使用场景 |
|------|------|----------|
| 智能对话 | 与模型进行自然语言对话 | 日常聊天、知识问答、创意生成 |
| 思考过程显示 | 展示模型的思考过程 | 理解模型推理逻辑、教育学习 |
| 多模型支持 | 支持多种规格的模型 | 适应不同硬件条件 |
| 模型管理 | 提供模型选择和状态管理 | 优化性能、解决问题 |
| 聊天记录管理 | 支持清除聊天记录 | 保持界面整洁、保护隐私 |

### 2.2 技术特性

- **模块化设计**：代码结构清晰，易于维护和扩展
- **缓存机制**：实现模型列表缓存，减少网络请求
- **自动刷新**：定期自动刷新模型列表，确保数据最新
- **错误处理**：完善的错误处理机制，提供友好的错误提示
- **会话管理**：使用 Streamlit 会话状态，实现数据持久化

## 3. 系统要求

### 3.1 硬件要求

| 模型规格 | 推荐 CPU | 推荐 GPU | 推荐内存 |
|---------|----------|----------|----------|
| 8B | 4核+ | 4GB 显存+ | 16GB+ |
| 14B | 6核+ | 8GB 显存+ | 32GB+ |
| 32B | 8核+ | 16GB 显存+ | 64GB+ |
| 64B | 12核+ | 24GB 显存+ | 128GB+ |

### 3.2 软件要求

- **操作系统**：Windows 10+, macOS 10.15+, Linux
- **Python**：3.8+
- **Ollama**：最新版本
- **依赖包**：详见 `requirements.txt`

## 4. 安装与配置

### 4.1 安装步骤

1. **安装 Ollama**
   - 从 [Ollama 官网](https://ollama.com/download) 下载并安装
   - 启动 Ollama 服务

2. **拉取模型**
   ```bash
   # 拉取基础模型
   ollama pull deepseek-r1:8b
   ```

3. **安装项目依赖**
   ```bash
   # 进入项目目录
   cd streamlit_chat_assistant
   
   # 安装依赖
   pip install -r requirements.txt
   ```

4. **运行应用**
   ```bash
   # 启动 Streamlit 应用
   streamlit run streamlit_app.py
   ```

### 4.2 配置说明

| 配置项 | 说明 | 默认值 | 修改位置 |
|--------|------|--------|----------|
| 默认模型 | 应用启动时使用的模型 | deepseek-r1:8b | src/config.py |
| 可用模型 | 支持的模型列表 | 见配置文件 | src/config.py |
| Ollama 地址 | Ollama 服务地址 | http://localhost:11434 | src/config.py |
| 缓存时间 | 模型列表缓存时间 | 300秒 | src/config.py |

## 5. 使用指南

### 5.1 基本操作流程

1. **启动应用**：运行 `streamlit run streamlit_app.py`
2. **访问界面**：在浏览器中打开 `http://localhost:8501`
3. **选择模型**：在左侧"模型管理工具"中选择合适的模型
4. **开始对话**：在底部输入框中输入消息，按 Enter 发送
5. **查看回复**：等待模型生成回复和思考过程
6. **管理聊天**：使用"清除聊天记录"按钮管理对话历史

### 5.2 高级操作

#### 5.2.1 模型管理

- **切换模型**：在模型选择下拉菜单中选择不同的模型
- **重新加载模型**：当模型出现问题时，点击"重新加载"按钮
- **刷新模型列表**：当 Ollama 服务器上的模型发生变化时，点击"刷新模型列表"按钮

#### 5.2.2 性能优化

- **选择合适的模型**：根据硬件条件选择合适规格的模型
- **关闭不必要的程序**：释放系统资源
- **使用较小的模型**：在性能受限的情况下，选择 8B 模型

## 6. 故障排除

### 6.1 常见问题

| 问题 | 可能原因 | 解决方案 |
|------|----------|----------|
| 模型加载失败 | Ollama 服务未启动 | 启动 Ollama 服务 |
| 模型加载失败 | 模型未拉取 | 运行 `ollama pull deepseek-r1:8b` |
| 响应缓慢 | 硬件性能不足 | 选择较小的模型 |
| 响应缓慢 | 显卡显存不足 | 确保显卡显存足够 |
| 连接失败 | Ollama 服务端口问题 | 检查端口 11434 是否可用 |

### 6.2 错误提示

| 错误信息 | 含义 | 解决方案 |
|----------|------|----------|
| Model requires more memory | 模型需要更多内存 | 选择较小的模型 |
| Failed to connect to Ollama | 无法连接到 Ollama | 检查 Ollama 服务状态 |
| No such model | 模型不存在 | 拉取相应模型 |

### 6.3 日志查看

应用运行时的日志会显示在终端中，包含详细的错误信息和操作记录。当遇到问题时，查看终端日志可以帮助诊断问题。

## 7. 最佳实践

### 7.1 对话技巧

- **明确具体**：提出具体的问题，获得更准确的回答
- **提供上下文**：对于复杂问题，提供足够的背景信息
- **逐步追问**：对于复杂话题，采用逐步追问的方式
- **使用引导词**：使用 "请详细解释"、"举个例子" 等引导词

### 7.2 硬件优化

- **GPU 加速**：确保 Ollama 正确使用 GPU
- **内存管理**：关闭不必要的应用程序
- **模型选择**：根据硬件条件选择合适的模型
- **散热管理**：确保设备有良好的散热

### 7.3 安全使用

- **隐私保护**：避免在对话中输入敏感信息
- **数据安全**：定期清除聊天记录
- **内容验证**：对模型输出的信息进行验证
- **合理使用**：避免过度依赖模型输出

## 8. 扩展与定制

### 8.1 添加新模型

1. **拉取模型**：`ollama pull model_name:tag`
2. **修改配置**：在 `src/config.py` 中的 `AVAILABLE_MODELS` 列表中添加新模型
3. **重启应用**：重新运行 Streamlit 应用

### 8.2 界面定制

- **修改标题**：在 `src/config.py` 中修改 `APP_TITLE`
- **修改图标**：在 `src/config.py` 中修改 `APP_ICON`
- **调整布局**：在 `src/ui.py` 中修改界面组件

### 8.3 功能扩展

- **添加新功能**：在 `src` 目录中添加新的模块
- **集成其他服务**：在 `src/models.py` 中添加新的模型管理函数
- **添加插件**：利用 Streamlit 的插件系统扩展功能

## 9. 技术支持

### 9.1 获取帮助

1. **查看文档**：阅读本说明书和 `docs/usage.md`
2. **查看日志**：检查终端中的应用日志
3. **社区支持**：访问 Ollama 和 Streamlit 社区

### 9.2 贡献代码

欢迎通过以下方式贡献代码：
- **提交 Issue**：报告问题或提出建议
- **提交 Pull Request**：贡献代码改进
- **文档完善**：改进文档和使用说明

## 10. 版本历史

| 版本 | 日期 | 主要变更 |
|------|------|----------|
| 1.0.0 | 2026-02-10 | 初始版本，支持基本聊天功能 |

## 11. 项目流程图

### 11.1 完整功能流程图

项目包含一个完整的功能流程图，展示了从启动到对话的全部流程：

- **文件位置**：`docs/complete_flowchart.md`
- **查看方式**：使用支持 Mermaid 的 Markdown 查看器打开
- **包含流程**：启动流程、模型管理、对话流程、操作管理

### 11.2 流程图特点

- **完整闭环**：展示了从应用启动到对话结束的完整流程
- **逻辑清晰**：每个节点职责明确，决策点标记清晰
- **错误处理**：包含了模型连接失败等错误情况的处理流程
- **用户交互**：突出了用户操作和系统响应的交互过程
- **状态管理**：展示了会话状态和模型状态的管理流程

---

## 附录：术语表

| 术语 | 解释 |
|------|------|
| LLM | 大型语言模型，如 DeepSeek R1 |
| Ollama | 本地 LLM 服务，用于运行模型 |
| Streamlit | 用于创建数据应用的 Python 库 |
| 思考过程 | 模型在生成回复时的推理逻辑 |
| 8B/14B | 模型参数数量，B 表示十亿 |
| GPU | 图形处理单元，用于加速模型推理 |
| 显存 | GPU 内存，用于存储模型和中间计算结果 |
| 缓存 | 临时存储数据，减少重复计算或请求 |
