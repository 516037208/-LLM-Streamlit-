# 使用说明

本文档提供 Streamlit 智能聊天助手的详细使用指南。

## 快速开始

### 1. 安装和运行

1. **安装依赖**：
   ```bash
   pip install -r requirements.txt
   ```

2. **运行应用**：
   ```bash
   streamlit run streamlit_app.py
   ```

3. **访问应用**：
   在浏览器中打开 `http://localhost:8501`

### 2. 基本操作

1. **发送消息**：在底部输入框中输入消息，按 Enter 发送
2. **查看回复**：等待模型生成回复
3. **查看思考过程**：回复上方会显示模型的思考过程

## 功能详解

### 模型管理工具

**模型选择**
- 在左侧的"模型管理工具"中，通过下拉菜单选择不同的模型
- 支持的模型包括：`deepseek-r1:8b`、`deepseek-r1:14b`、`deepseek-r1:32b`、`deepseek-r1:64b`

**操作按钮**
- 🗑️ **清除聊天记录**：清空当前的聊天历史
- 🔄 **重新加载**：重置模型状态，解决模型异常问题
- 🔄 **刷新模型列表**：从 Ollama 服务器获取最新的模型列表

### 高级功能

**自动模型管理**
- 应用每5分钟自动刷新一次模型列表
- 确保您始终能看到 Ollama 服务器上的最新模型

**思考过程显示**
- 模型在生成回复时会显示其思考过程
- 帮助您理解模型的推理逻辑
- 增加对话的透明度和可信度

**错误处理**
- 当模型加载失败时，会显示详细的错误信息
- 提供"重试"按钮，方便快速恢复
- 优雅处理各种异常情况，确保应用稳定运行

## 常见问题

### 1. 模型加载失败

**可能原因**：
- Ollama 服务未启动
- 网络连接问题
- 模型未正确拉取

**解决方案**：
1. 检查 Ollama 服务状态
2. 确认网络连接正常
3. 运行 `ollama pull deepseek-r1:8b` 拉取模型
4. 点击"重新加载"按钮

### 2. 响应速度慢

**可能原因**：
- 模型参数较大（如 14B、32B、64B）
- 电脑硬件性能不足
- Ollama 服务负载过高

**解决方案**：
1. 选择较小的模型（如 8B）
2. 确保电脑有足够的内存和 CPU 资源
3. 等待 Ollama 服务负载降低

### 3. 无法连接到 Ollama

**可能原因**：
- Ollama 服务未启动
- 服务端口被占用
- 防火墙阻止连接

**解决方案**：
1. 启动 Ollama 服务
2. 检查端口 11434 是否可用
3. 暂时关闭防火墙测试

## 技术细节

### 项目结构

```
streamlit_chat_assistant/
├── streamlit_app.py       # 主应用入口
├── src/                   # 源代码
│   ├── config.py          # 配置管理
│   ├── session.py         # 会话状态管理
│   ├── models.py          # 模型管理
│   ├── messages.py        # 消息管理
│   └── ui.py              # 界面组件
├── tests/                 # 测试文件
├── docs/                  # 文档
└── examples/              # 示例
```

### 配置文件

**src/config.py**
- `DEFAULT_MODEL`：默认模型
- `AVAILABLE_MODELS`：可用模型列表
- `OLLAMA_HOST`：Ollama 服务器地址
- `CACHE_DURATION`：缓存有效期（秒）
- `APP_TITLE`：应用标题
- `APP_ICON`：应用图标
- `APP_LAYOUT`：应用布局

### 会话状态

应用使用 Streamlit 的会话状态管理以下数据：
- `messages`：聊天记录
- `ollama_model`：模型加载状态
- `Model_error`：错误信息
- `client`：Ollama 客户端实例
- `select_model`：当前选择的模型
- `model_cache`：模型列表缓存
- `cache_time`：缓存时间戳
- `last_refresh_time`：上次刷新时间

## 扩展与定制

### 添加新模型

在 `src/config.py` 中的 `AVAILABLE_MODELS` 列表中添加新模型名称。

### 自定义界面

修改 `src/ui.py` 中的相关函数，定制界面布局和样式。

### 集成其他服务

在 `src/models.py` 中添加新的模型管理函数，集成其他 LLM 服务。

## 性能优化

1. **选择合适的模型**：根据硬件性能选择合适大小的模型
2. **减少模型切换**：频繁切换模型会增加加载时间
3. **合理使用缓存**：应用已实现模型列表缓存，减少网络请求
4. **优化电脑性能**：关闭不必要的程序，释放系统资源

## 安全注意事项

1. **敏感信息**：请勿在聊天中输入敏感信息，如密码、信用卡号等
2. **网络安全**：确保 Ollama 服务只在本地或安全网络中运行
3. **模型输出**：模型输出可能不准确，请谨慎使用

## 联系与支持

如果您在使用过程中遇到问题，请：

1. 检查本文档的"常见问题"部分
2. 查看应用运行日志，了解详细错误信息
3. 确认 Ollama 服务正常运行
4. 尝试重新启动应用

---

希望本使用说明能帮助您更好地使用 Streamlit 智能聊天助手！
